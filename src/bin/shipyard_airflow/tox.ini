[tox]
skipsdist=True
envlist = py310, pep8, bandit

[testenv]
setenv=
  PYTHONWARNING=all
  LANG=C.UTF-8
  LC_ALL=C.UTF-8
deps =
  -r{toxinidir}/requirements-frozen.txt
  -r{toxinidir}/test-requirements.txt
[testenv:freeze]
recreate = True
allowlist_externals=
  rm
  sh
deps=
  -r{toxinidir}/requirements-direct.txt
  -c https://raw.githubusercontent.com/apache/airflow/constraints-3.0.2/constraints-3.10.txt
commands=
  rm -f {toxinidir}/requirements-frozen.txt
  sh -c "pip freeze --all | grep -vE 'shipyard_airflow|pyinotify|pkg-resources' > requirements-frozen.txt"

[testenv:safety]
basepython = python3
deps =
  safety
allowlist_externals=
  safety
commands =
  safety check -r {toxinidir}/requirements-frozen.txt  --full-report

[testenv:py310]
skipsdist=True
setenv=
  SLUGIFY_USES_TEXT_UNIDECODE=yes
  PYTHONWARNINGS=ignore::DeprecationWarning,ignore::FutureWarning
  ; Core Section
  AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS = True
  ; API Section
  AIRFLOW__API__AUTH_BACKENDS = airflow.api.auth.backend.default
basepython=python3.10
allowlist_externals=
  bash
  airflow
  ../../../tools/wait_for_dag_activation.sh
  ../../../tools/wait_for_dag_processor_start.sh
  ../../../tools/airflow_process_manager.sh
commands =
  ../../../tools/airflow_process_manager.sh stop
  bash -c "rm -rf $HOME/airflow"
  airflow version
  airflow db migrate
  bash -c "nohup airflow dag-processor >/dev/null 2>&1 & echo $! > /tmp/dag-processor.pid"
  bash -c "nohup airflow api-server >/dev/null 2>&1 & echo $! > /tmp/api-server.pid"
  bash -c "nohup airflow scheduler >/dev/null 2>&1 & echo $! > /tmp/scheduler.pid"
  ../../../tools/wait_for_dag_processor_start.sh
  airflow info
  airflow dags list
  ; airflow dags list-import-errors
  airflow dags unpause example_bash_operator
  ../../../tools/wait_for_dag_activation.sh
  airflow dags test example_bash_operator
  airflow backfill create --dag-id example_bash_operator --from-date 2018-01-01 --to-date 2018-01-02
  airflow dags list-runs example_bash_operator
  airflow tasks states-for-dag-run example_bash_operator "2018-01-01T00:00:00Z"
  airflow dags state example_bash_operator "2018-01-01T00:00:00Z"
  pytest {posargs} -vv


[testenv:integration]
skipsdist=True
setenv=
  SLUGIFY_USES_TEXT_UNIDECODE=yes
  PYTHONWARNINGS=ignore::DeprecationWarning,ignore::FutureWarning

  ; Core Section
  AIRFLOW__CORE__DAGS_FOLDER = ~/airflow/dags
  AIRFLOW__CORE__EXECUTOR = CeleryExecutor
  AIRFLOW__CORE__FERNET_KEY = fKp7omMJ4QlTxfZzVBSiyXVgeCK-6epRjGgMpEIsjvs=
  AIRFLOW__CORE__PARALLELISM = 32
  AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG = 8
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION = False
  AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS = True
  AIRFLOW__CORE__KILLED_TASK_CLEANUP_TIME = 60
  AIRFLOW__CORE__DAG_RUN_CONF_OVERRIDES_PARAMS = False
  AIRFLOW__CORE__LOAD_EXAMPLES = False
  AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS = True
  AIRFLOW__CORE__EXECUTION_API_SERVER_URL = http://localhost:8080/execution/

  ; Database Section
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:password@localhost:5432/airflow

  ; API Section
  AIRFLOW__API__AUTH_BACKENDS = airflow.api.auth.backend.jwt
  AIRFLOW__API__BASE_URL = http://localhost
  AIRFLOW__API__HOST = 127.0.0.1
  AIRFLOW__API__PORT = 8080
  AIRFLOW__API__WORKERS = 4
  AIRFLOW__API__WORKER_TIMEOUT = 120
  AIRFLOW__API__SSL_CERT =
  AIRFLOW__API__SSL_KEY =
  AIRFLOW__API__ACCESS_LOGFILE = -
  AIRFLOW__API_AUTH__JWT_SECRET = Rtx8fOKxn3QtxLp5GCI0CFx2rfCd5NStUY+5GZIGGWo=
  AIRFLOW__API_AUTH__JWT_ALGORITHM = HS512
  AIRFLOW__API_AUTH__JWT_ISSUER = airship
  AIRFLOW__API_AUTH__JWT_AUDIENCE = apache-airflow
  AIRFLOW__EXECUTION_API__JWT_AUDIENCE = apache-airflow
  AIRFLOW__EXECUTION_API__JWT_EXPIRATION_TIME = 600
  AIRFLOW__API_AUTH__JWT_EXPIRATION_TIME = 600

  ; DAG Processor section
  AIRFLOW__DAG_PROCESSOR__MIN_FILE_PROCESS_INTERVAL = 864000
  AIRFLOW__DAG_PROCESSOR__PRINT_STATS_INTERVAL = 0

  ; Logging Section
  AIRFLOW__LOGGING__BASE_LOG_FOLDER = ~/airflow/logs
  AIRFLOW__LOGGING__LOG_FILENAME_TEMPLATE = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ti.logical_date.strftime('%Y-%m-%dT%H:%M:%S') }}/{{ ti.try_number }}.log
  AIRFLOW__LOGGING__LOGGING_LEVEL = INFO
  AIRFLOW__LOGGING__DAG_PROCESSOR_LOGGING_LEVEL = INFO
  AIRFLOW__LOGGING__CELERY_LOGGING_LEVEL = INFO
  AIRFLOW__LOGGING__LOGGING_CONFIG_CLASS = config.log_config.DEFAULT_LOGGING_CONFIG
  AIRFLOW__LOGGING__DAG_LOGGING_LEVEL = INFO
  AIRFLOW__LOGGING__FAB_LOGGING_LEVEL = INFO
  AIRFLOW__LOGGING__LOG_FORMAT = %%(asctime)s %%(levelname)-8s %%(filename)s:%%(lineno)3d:%%(funcName)s %%(module)s %%(message)s

  ; Celery Section
  AIRFLOW__CELERY__BROKER_URL = amqp://airflow:password@localhost:5672/
  AIRFLOW__CELERY__RESULT_BACKEND = db+postgresql://airflow:password@localhost:5432/airflow
  AIRFLOW__CELERY__WORKER_CONCURRENCY = 16
  AIRFLOW__CELERY__TASK_TRACK_STARTED = True
  AIRFLOW__CELERY__TASK_TIME_LIMIT = 300
basepython=python3.10
allowlist_externals=
  bash
  airflow
  python3
  ../../../tools/airflow_process_manager.sh
  ../../../tools/send_dag_run.py
  ../../../tools/airflow_tasks_log_reader.sh
  ../../../tools/wait_for_dag_completion.sh
commands=
  ../../../tools/airflow_process_manager.sh stop
  pip3 install ./ --use-pep517
  bash -c "rm -rf $HOME/airflow"
  bash -c "env | grep AIRFLOW"
  bash -c "mkdir $HOME/airflow/"
  bash -c "cp -a shipyard_airflow/dags $HOME/airflow/"
  bash -c "cp -a ../../../images/airflow/config .tox/integration/lib/python3.10/site-packages/"
  bash -c "cp -a shipyard_airflow/dags $HOME/airflow/"
  bash -c "cp -a shipyard_airflow/plugins $HOME/airflow/"
  ../../../tools/airflow_process_manager.sh start
  airflow version
  airflow info
  airflow dags list
  ; airflow dags list-import-errors
  ../../../tools/send_dag_run.py
  airflow dags list
  airflow dags list-runs update_software
  ../../../tools/wait_for_dag_completion.sh update_software
  bash -c "../../../tools/airflow_tasks_log_reader.sh $HOME/airflow/logs/update_software"

[testenv:cover]
skipsdist=True
setenv=
  SLUGIFY_USES_TEXT_UNIDECODE=yes
  PYTHONWARNINGS=ignore::DeprecationWarning,ignore::FutureWarning
  ; Core Section
  AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS = True
  ; API Section
  AIRFLOW__API__AUTH_BACKENDS = airflow.api.auth.backend.default
  ; Log section
  AIRFLOW__DAG_PROCESSOR__PRINT_STATS_INTERVAL = 0
allowlist_externals=
  bash
  airflow
  ../../../tools/wait_for_dag_activation.sh
  ../../../tools/wait_for_dag_processor_start.sh
  ../../../tools/airflow_process_manager.sh
commands =
  ../../../tools/airflow_process_manager.sh stop
  bash -c "rm -rf $HOME/airflow"
  airflow version
  airflow db migrate
  bash -c "nohup airflow dag-processor >/dev/null 2>&1 & echo $! > /tmp/dag-processor.pid"
  bash -c "nohup airflow api-server >/dev/null 2>&1 & echo $! > /tmp/api-server.pid"
  bash -c "nohup airflow scheduler >/dev/null 2>&1 & echo $! > /tmp/scheduler.pid"
  ../../../tools/wait_for_dag_processor_start.sh
  airflow info
  airflow dags list
  ; airflow dags list-import-errors
  airflow dags unpause example_bash_operator
  ../../../tools/wait_for_dag_activation.sh
  airflow dags test example_bash_operator
  airflow backfill create --dag-id example_bash_operator --from-date 2018-01-01 --to-date 2018-01-02
  airflow dags list-runs example_bash_operator
  airflow tasks states-for-dag-run example_bash_operator "2018-01-01T00:00:00Z"
  airflow dags state example_bash_operator "2018-01-01T00:00:00Z"
  pytest \
    {posargs} \
    --cov-branch \
    --cov-report=term-missing:skip-covered \
    --cov-config=.coveragerc \
    --cov=shipyard_airflow \
    --cov-report html:cover \
    --cov-report xml:cover/coverage.xml \
    --cov-report term \
    -vv

[testenv:yapf]
allowlist_externals=find
deps=
  yapf
commands=
  yapf -i -r  --style={toxinidir}/.style.yapf {toxinidir}/shipyard_airflow
  find {toxinidir}/shipyard_airflow -name '__init__.py' -exec yapf -i --style={toxinidir}/.style.yapf \{\} ;

[testenv:pep8]
deps =
  ruff
  flake8
  bandit
commands =
  ruff check shipyard_airflow/dags/ --select AIR301,AIR302 --preview
  flake8 {toxinidir}/shipyard_airflow
  bandit -r shipyard_airflow

[testenv:bandit]
skipsdist= True
commands =
  bandit -r shipyard_airflow -n 5

[testenv:ruff]
skipsdist= True
deps =
  ruff
commands =
tests/unit/control/test_actions_api.py::test_invoke_airflow_dag_succes  ruff check shipyard_airflow/dags/ --select AIR301,AIR302,AIR30,AIR31 --preview

[testenv:genconfig]
setenv=
  SLUGIFY_USES_TEXT_UNIDECODE=yes
commands =
    pip install . --use-pep517
    oslo-config-generator --config-file=generator/config-generator.conf

[testenv:genpolicy]
setenv=
  SLUGIFY_USES_TEXT_UNIDECODE=yes
commands =
    pip install . --use-pep517
    oslopolicy-sample-generator --config-file=generator/policy-generator.conf

[testenv:gen_all]
# Combined to make these run together instead of setting up separately
skipsdist=True
setenv=
  SLUGIFY_USES_TEXT_UNIDECODE=yes
commands =
    pip install . --use-pep517
    oslo-config-generator --config-file=generator/config-generator.conf
    oslopolicy-sample-generator --config-file=generator/policy-generator.conf

[flake8]
filename = *.py
# NOTE(Bryan Strassner) ignoring F841 because of the airflow example pattern
#     of naming variables even if they aren't used for DAGs and Operators.
#     Doing so adds readability and context in this case.
# TODO(Bryan Strassner) The hacking rules defined as ignored below in many
#     cases need to be un-ignored and fixed up. These are ignored because of
#     the method in which test requirements bring in the hacking rules from
#     other projects.
# W504 line break after binary operator
# TODO(rb560u): Address E722 violations
ignore = F841, H101, H201, H210, H238, H301, H304, H306, H401, H403, H404, H405, W504, E722
# NOTE(Bryan Strassner) excluding 3rd party and generated code that is brought into the
#     codebase.
exclude = .venv,.git,.tox,build,dist,*lib/python*,*egg,alembic/env.py,docs
